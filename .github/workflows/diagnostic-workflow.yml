name: ðŸ” Diagnostic - Complete Data Analysis

on:
  workflow_dispatch:

jobs:
  diagnose:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: System libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libgdal-dev libgeos-dev libproj-dev libudunits2-dev jq curl

      - name: Install R packages
        run: |
          Rscript -e 'install.packages(c("sf", "data.table"), repos="https://cloud.r-project.org")'

      # ================================================================
      # STEP 1: Get Fresh Token
      # ================================================================
      - name: Get Fresh Dropbox Access Token
        id: get_token
        env:
          DROPBOX_REFRESH_TOKEN: ${{ secrets.DROPBOX_REFRESH_TOKEN }}
          DROPBOX_APP_KEY: ${{ secrets.DROPBOX_APP_KEY }}
          DROPBOX_APP_SECRET: ${{ secrets.DROPBOX_APP_SECRET }}
        run: |
          echo "ðŸ”‘ Getting fresh access token..."
          
          RESPONSE=$(curl -s -X POST https://api.dropbox.com/oauth2/token \
            -u "${DROPBOX_APP_KEY}:${DROPBOX_APP_SECRET}" \
            -d "grant_type=refresh_token&refresh_token=${DROPBOX_REFRESH_TOKEN}")
          
          ACCESS_TOKEN=$(echo "$RESPONSE" | jq -r '.access_token // empty')
          
          if [ -z "$ACCESS_TOKEN" ]; then
            echo "âŒ Failed to get access token"
            echo "$RESPONSE" | jq '.'
            exit 1
          fi
          
          echo "âœ… Token obtained"
          echo "access_token=$ACCESS_TOKEN" >> $GITHUB_OUTPUT

      # ================================================================
      # STEP 2: Analyze Dropbox Structure
      # ================================================================
      - name: Analyze Dropbox File Structure
        env:
          DROPBOX_TOKEN: ${{ steps.get_token.outputs.access_token }}
        run: |
          echo ""
          echo "========================================="
          echo "ðŸ“ DROPBOX FILE STRUCTURE ANALYSIS"
          echo "========================================="
          echo ""
          
          # Use CORRECT path
          DROPBOX_PATH="/hq-outages-uploader-appfolder/hq-outages"
          
          echo "Listing files from: $DROPBOX_PATH"
          echo ""
          
          # Get files with pagination
          ALL_FILES=""
          HAS_MORE=true
          CURSOR=""
          PAGE=1
          
          while [ "$HAS_MORE" = "true" ] && [ $PAGE -le 5 ]; do
            echo "ðŸ“„ Page $PAGE..."
            
            if [ -z "$CURSOR" ]; then
              RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Content-Type: application/json" \
                -d "{\"path\": \"$DROPBOX_PATH\", \"recursive\": true}")
            else
              RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder/continue \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Content-Type: application/json" \
                -d "{\"cursor\": \"$CURSOR\"}")
            fi
            
            if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
              echo "âŒ Error:"
              echo "$RESPONSE" | jq '.error'
              exit 1
            fi
            
            PAGE_FILES=$(echo "$RESPONSE" | jq -r '.entries[]? | select(.".tag" == "file") | .path_display' || true)
            
            if [ ! -z "$PAGE_FILES" ]; then
              if [ -z "$ALL_FILES" ]; then
                ALL_FILES="$PAGE_FILES"
              else
                ALL_FILES="$ALL_FILES"$'\n'"$PAGE_FILES"
              fi
            fi
            
            HAS_MORE=$(echo "$RESPONSE" | jq -r '.has_more')
            if [ "$HAS_MORE" = "true" ]; then
              CURSOR=$(echo "$RESPONSE" | jq -r '.cursor')
              PAGE=$((PAGE + 1))
            fi
          done
          
          echo ""
          echo "âœ… Retrieved file list"
          
          # Filter polygon files
          POLYGONS=$(echo "$ALL_FILES" | grep "/polygons_.*\.geojson$" || true)
          
          POLY_COUNT=$(echo "$POLYGONS" | wc -l 2>/dev/null | tr -d ' ')
          echo ""
          echo "ðŸ“Š Total polygon files: $POLY_COUNT"
          
          if [ "$POLY_COUNT" -eq 0 ]; then
            echo "âŒ No polygon files found!"
            exit 1
          fi
          
          # Analyze by date
          echo ""
          echo "ðŸ“… FILES PER DATE:"
          echo "$POLYGONS" | sed 's/.*date=\([0-9-]*\).*/\1/' | sort | uniq -c | sort -rn | while read count date; do
            echo "  $date: $count files"
          done
          
          # Analyze by hour (for one date)
          echo ""
          echo "â° HOUR DISTRIBUTION (sample date):"
          SAMPLE_DATE=$(echo "$POLYGONS" | sed 's/.*date=\([0-9-]*\).*/\1/' | sort -u | head -1)
          echo "  Date: $SAMPLE_DATE"
          echo "$POLYGONS" | grep "date=$SAMPLE_DATE" | sed 's/.*hour=\([0-9]*\).*/\1/' | sort -n | uniq -c | while read count hour; do
            printf "  Hour %02d: %d file(s)\n" $hour $count
          done
          
          # Save list for next step
          echo "$POLYGONS" > /tmp/all_polygons.txt
          
          echo ""
          echo "========================================="

      # ================================================================
      # STEP 3: Download Sample Files
      # ================================================================
      - name: Download Sample Files
        env:
          DROPBOX_TOKEN: ${{ steps.get_token.outputs.access_token }}
        run: |
          echo ""
          echo "========================================="
          echo "â¬‡ï¸  DOWNLOADING SAMPLE FILES"
          echo "========================================="
          echo ""
          
          mkdir -p data/daily
          
          # Download files from last 2 dates
          RECENT_DATES=$(cat /tmp/all_polygons.txt | sed 's/.*date=\([0-9-]*\).*/\1/' | sort -u | tail -2)
          
          DOWNLOADED=0
          FAILED=0
          
          for date in $RECENT_DATES; do
            echo "ðŸ“… Date: $date"
            
            # Get files for this date (max 30 per date)
            DATE_FILES=$(cat /tmp/all_polygons.txt | grep "date=$date" | head -30)
            
            for file in $DATE_FILES; do
              fname=$(basename "$file")
              
              if curl -s -f -X POST https://content.dropboxapi.com/2/files/download \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Dropbox-API-Arg: {\"path\": \"$file\"}" \
                -o "data/daily/$fname" 2>/dev/null && [ -s "data/daily/$fname" ]; then
                
                ((DOWNLOADED++))
                [ $DOWNLOADED -le 5 ] && echo "  âœ“ $fname"
              else
                rm -f "data/daily/$fname"
                ((FAILED++))
              fi
              
              # Limit downloads
              if [ $DOWNLOADED -ge 50 ]; then
                break 2
              fi
            done
          done
          
          echo ""
          echo "ðŸ“Š Downloaded: $DOWNLOADED files"
          echo "âŒ Failed: $FAILED files"
          echo ""
          echo "========================================="

      # ================================================================
      # STEP 4: Analyze Downloaded Files
      # ================================================================
      - name: Analyze Source Files (R)
        run: |
          echo ""
          echo "========================================="
          echo "ðŸ”¬ SOURCE FILE ANALYSIS"
          echo "========================================="
          echo ""
          
          cat > analyze_source.R << 'EOF'
suppressPackageStartupMessages(library(sf))
suppressPackageStartupMessages(library(data.table))

files <- list.files("data/daily", pattern = "^polygons_.*\\.geojson$", full.names = TRUE)

if (length(files) == 0) {
  cat("âŒ No files downloaded!\n")
  quit(status = 1)
}

cat(sprintf("Analyzing %d downloaded files\n\n", length(files)))

# Parse timestamps
basenames <- basename(files)
timestamps <- regmatches(basenames, regexpr("\\d{8}t\\d{6}", basenames, ignore.case = TRUE))

dates <- sprintf("%s-%s-%s", 
                substr(timestamps, 1, 4), 
                substr(timestamps, 5, 6), 
                substr(timestamps, 7, 8))

hours <- as.integer(substr(timestamps, 10, 11))

# Create data table
files_dt <- data.table(
  file = files,
  date = dates,
  hour = hours,
  timestamp = timestamps
)

cat("========================================\n")
cat("FILES PER DATE:\n")
cat("========================================\n")
files_per_date <- files_dt[, .N, by = date][order(-N)]
print(files_per_date)

cat("\n========================================\n")
cat("HOUR DISTRIBUTION:\n")
cat("========================================\n")
hour_dist <- table(hours)
print(hour_dist)

cat("\n========================================\n")
cat("SAMPLE FILE CONTENTS:\n")
cat("========================================\n\n")

for (i in 1:min(3, length(files))) {
  f <- files[i]
  cat(sprintf("[%d] %s\n", i, basename(f)))
  cat(sprintf("    Date: %s, Hour: %02d\n", dates[i], hours[i]))
  
  tryCatch({
    data <- st_read(f, quiet = TRUE)
    cat(sprintf("    Polygons: %d\n", nrow(data)))
    cat(sprintf("    Columns: %s\n", paste(names(data)[1:min(5, length(names(data)))], collapse=", ")))
    
    # Check area
    if (nrow(data) > 0) {
      area <- st_area(st_transform(data, 32618))
      cat(sprintf("    Total area: %.0f kmÂ²\n", sum(area) / 1e6))
    }
  }, error = function(e) {
    cat(sprintf("    âŒ Error: %s\n", e$message))
  })
  cat("\n")
}

cat("========================================\n")
cat("DIAGNOSTIC RESULTS:\n")
cat("========================================\n\n")

max_files_per_date <- max(files_per_date$N)
min_files_per_date <- min(files_per_date$N)

cat(sprintf("Files per date: min=%d, max=%d\n\n", min_files_per_date, max_files_per_date))

if (max_files_per_date == 1) {
  cat("âŒ PROBLEM: Only 1 file per date!\n")
  cat("   Your hourly scraper is not running every hour.\n")
  cat("   Result: Daily summaries will show count=1 for everything.\n\n")
} else if (max_files_per_date < 20) {
  cat(sprintf("âš ï¸  WARNING: Only %d files per date\n", max_files_per_date))
  cat("   Expected ~24 for full hourly coverage.\n")
  cat("   Some hours may be missing.\n\n")
} else {
  cat(sprintf("âœ… GOOD: %d files per date\n", max_files_per_date))
  cat("   Hourly scraping is working!\n\n")
}

# Deduplication check
setkey(files_dt, date, hour)
deduplicated <- files_dt[, .SD[.N], by = .(date, hour)]

if (nrow(deduplicated) < nrow(files_dt)) {
  duplicates <- nrow(files_dt) - nrow(deduplicated)
  cat(sprintf("ðŸ“‹ Duplicate hours detected: %d files would be deduplicated\n", duplicates))
  cat("   (This is normal if you have multiple files per hour)\n\n")
}

cat("After deduplication:\n")
cat(sprintf("  Total files: %d\n", nrow(deduplicated)))
cat(sprintf("  Unique dates: %d\n", length(unique(deduplicated$date))))
cat(sprintf("  Avg files per date: %.1f\n", nrow(deduplicated) / length(unique(deduplicated$date))))

EOF
          
          Rscript analyze_source.R

      # ================================================================
      # STEP 5: Check Processed Summaries (if exist)
      # ================================================================
      - name: Download Existing Processed Summaries
        continue-on-error: true
        run: |
          echo ""
          echo "========================================="
          echo "ðŸ“¥ CHECKING EXISTING PROCESSED DATA"
          echo "========================================="
          echo ""
          
          # Clone public repo to see what's deployed
          if git clone --depth 1 --branch gh-pages \
            https://github.com/evelynebrie/hq-outages-public.git temp-public 2>/dev/null; then
            
            echo "âœ… Downloaded data from public repo"
            
            mkdir -p public/daily public/total
            
            if [ -d "temp-public/daily" ]; then
              cp temp-public/daily/*.geojson public/daily/ 2>/dev/null || true
              DAILY_COUNT=$(ls -1 public/daily/*.geojson 2>/dev/null | wc -l)
              echo "  Daily summaries: $DAILY_COUNT files"
            fi
            
            if [ -d "temp-public/total" ]; then
              cp temp-public/total/*.geojson public/total/ 2>/dev/null || true
              echo "  Cumulative total: copied"
            fi
            
            rm -rf temp-public
          else
            echo "âš ï¸  Could not access public repo"
          fi

      - name: Analyze Processed Summaries (R)
        continue-on-error: true
        run: |
          echo ""
          echo "========================================="
          echo "ðŸ”¬ PROCESSED SUMMARY ANALYSIS"
          echo "========================================="
          echo ""
          
          cat > analyze_processed.R << 'EOF'
suppressPackageStartupMessages(library(sf))

daily_files <- list.files("public/daily", pattern = "^daily_.*\\.geojson$", full.names = TRUE)

if (length(daily_files) == 0) {
  cat("âš ï¸  No processed daily summaries found\n")
  cat("   (This is expected if workflow hasn't run yet)\n")
  quit()
}

cat(sprintf("Found %d processed daily summaries\n\n", length(daily_files)))

cat("========================================\n")
cat("DAILY SUMMARY ANALYSIS:\n")
cat("========================================\n\n")

for (i in 1:min(5, length(daily_files))) {
  f <- daily_files[i]
  date <- sub(".*daily_(.*)\\.geojson", "\\1", basename(f))
  
  tryCatch({
    data <- st_read(f, quiet = TRUE)
    
    cat(sprintf("[%d] %s\n", i, date))
    cat(sprintf("    Hexagons: %d\n", nrow(data)))
    
    if ("count" %in% names(data)) {
      cat(sprintf("    Hours/hex: min=%d, max=%d, mean=%.1f\n", 
                  min(data$count), max(data$count), mean(data$count)))
      
      if (max(data$count) == 1 && min(data$count) == 1) {
        cat("    âš ï¸  All hexes have count=1 (only 1 hour of data)\n")
      }
    }
    cat("\n")
  }, error = function(e) {
    cat(sprintf("    âŒ Error: %s\n\n", e$message))
  })
}

# Check cumulative
total_file <- "public/total/total_exposure.geojson"

if (file.exists(total_file)) {
  cat("\n========================================\n")
  cat("CUMULATIVE TOTAL ANALYSIS:\n")
  cat("========================================\n\n")
  
  total <- st_read(total_file, quiet = TRUE)
  
  cat(sprintf("Total hexagons: %d\n", nrow(total)))
  
  if ("hours_count" %in% names(total) && "days_affected" %in% names(total)) {
    max_hours <- max(total$hours_count)
    max_days <- max(total$days_affected)
    theoretical_max <- max_days * 24
    
    cat(sprintf("Max hours: %d\n", max_hours))
    cat(sprintf("Max days: %d\n", max_days))
    cat(sprintf("Theoretical max: %d\n", theoretical_max))
    
    if (max_hours > theoretical_max + 1) {
      cat(sprintf("\nâŒ BUG DETECTED: %d hours exceeds %d theoretical max!\n", max_hours, theoretical_max))
      cat("   This confirms double-counting issue.\n")
    } else {
      cat("\nâœ… Hours are within expected range\n")
    }
  }
}
EOF
          
          Rscript analyze_processed.R

      # ================================================================
      # STEP 6: Summary Report
      # ================================================================
      - name: Generate Diagnostic Summary
        if: always()
        run: |
          echo ""
          echo "========================================="
          echo "ðŸ“‹ DIAGNOSTIC SUMMARY"
          echo "========================================="
          echo ""
          
          echo "This diagnostic checked:"
          echo ""
          echo "1ï¸âƒ£  Dropbox Connection"
          echo "    âœ“ Token refresh"
          echo "    âœ“ File listing"
          echo "    âœ“ Correct path: /hq-outages-uploader-appfolder/hq-outages"
          echo ""
          echo "2ï¸âƒ£  Source Data Quality"
          echo "    âœ“ Files per date"
          echo "    âœ“ Hour distribution"
          echo "    âœ“ File contents"
          echo ""
          echo "3ï¸âƒ£  Processed Data (if exists)"
          echo "    âœ“ Daily summaries"
          echo "    âœ“ Cumulative total"
          echo "    âœ“ Double-counting check"
          echo ""
          echo "========================================="
          echo "NEXT STEPS:"
          echo "========================================="
          echo ""
          echo "Review the output above and look for:"
          echo ""
          echo "âœ… GOOD SIGNS:"
          echo "  â€¢ 20-24 files per date"
          echo "  â€¢ Hours 0-23 represented"
          echo "  â€¢ Max hours â‰¤ max days Ã— 24"
          echo ""
          echo "âŒ BAD SIGNS:"
          echo "  â€¢ Only 1 file per date â†’ Hourly scraping not working"
          echo "  â€¢ Max hours > max days Ã— 24 â†’ Double-counting bug"
          echo "  â€¢ All count=1 â†’ Missing hourly data"
          echo ""
          echo "If you see bad signs, apply the fixes from:"
          echo "  â€¢ publish-cumulative-CORRECT-PATH.yml"
          echo "  â€¢ build_cumulative_hex_FIXED.R"
          echo ""
          echo "========================================="
