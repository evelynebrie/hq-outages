name: Publish cumulative exposure - FAST incremental

on:
  schedule:
    - cron: '30 3 * * *'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: System libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libgdal-dev libgeos-dev libproj-dev libudunits2-dev jq curl

      - name: Cache R packages
        uses: actions/cache@v3
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-r-packages-v2
          restore-keys: ${{ runner.os }}-r-

      - name: Install R packages
        run: |
          Rscript -e 'pkgs <- c("sf","dplyr","fs","purrr","jsonlite","data.table"); inst <- setdiff(pkgs, rownames(installed.packages())); if(length(inst)) { cat("Installing", length(inst), "packages\n"); install.packages(inst, repos="https://cloud.r-project.org", Ncpus=2) } else { cat("All packages cached\n") }'

      - name: Restore data cache
        uses: actions/cache@v3
        with:
          path: data/daily/
          key: hq-daily-data-${{ github.run_id }}
          restore-keys: hq-daily-data-

      - name: Download NEW files from Dropbox
        env:
          DROPBOX_TOKEN: ${{ secrets.DROPBOX_TOKEN }}
        run: |
          set -eo pipefail
          
          echo "Listing Dropbox files..."
          
          # Make API call and save full response
          RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder \
            -H "Authorization: Bearer $DROPBOX_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"path": "/hq-outages", "recursive": true}')
          
          # Check for API errors first
          if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
            echo "‚ùå Error from Dropbox API:"
            echo "$RESPONSE" | jq '.error'
            exit 1
          fi
          
          # Check if entries field exists (even if empty array)
          if ! echo "$RESPONSE" | jq -e '.entries' > /dev/null 2>&1; then
            echo "‚ùå Unexpected response format (no .entries field)"
            echo "$RESPONSE"
            exit 1
          fi
          
          # Count total entries
          ENTRY_COUNT=$(echo "$RESPONSE" | jq '.entries | length')
          echo "Found $ENTRY_COUNT total entries in Dropbox"
          
          # Parse files - handle empty array gracefully
          FILES=$(echo "$RESPONSE" | jq -r '.entries[]? | select(.".tag" == "file") | .path_display' || true)
          
          # If no files found, check if folder is just empty or if there's a real problem
          if [ -z "$FILES" ]; then
            if [ "$ENTRY_COUNT" -eq 0 ]; then
              echo "‚ö†Ô∏è  Folder is empty - this is expected if scraping failed"
              echo "‚úì No new files to download, continuing with cached data..."
              mkdir -p data/daily
              # Count cached files
              CACHED_COUNT=$(find data/daily -name "*.geojson" 2>/dev/null | wc -l)
              echo "Using $CACHED_COUNT cached files from previous runs"
              
              # Continue to R analysis with existing cache
              exit 0
            else
              echo "‚ö†Ô∏è  Folder has $ENTRY_COUNT entries but no files found"
              echo "This might indicate folders only, which is OK"
              mkdir -p data/daily
              exit 0
            fi
          fi
          
          # Extract polygons and CSV files
          POLYGONS=$(echo "$FILES" | grep "polygons_.*\.geojson$" || true)
          CSV=$(echo "$FILES" | grep "outages_joined_.*\.csv$" || true)
          
          # Count relevant files (safe way without grep -c)
          POLY_COUNT=0
          if [ ! -z "$POLYGONS" ]; then
            POLY_COUNT=$(echo "$POLYGONS" | wc -l)
          fi
          
          CSV_COUNT=0
          if [ ! -z "$CSV" ]; then
            CSV_COUNT=$(echo "$CSV" | wc -l)
          fi
          
          TOTAL=$((POLY_COUNT + CSV_COUNT))
          
          echo "Found $TOTAL relevant files ($POLY_COUNT polygons, $CSV_COUNT CSV)"
          
          if [ "$TOTAL" -eq 0 ]; then
            echo "‚ö†Ô∏è  No polygon or CSV files found"
            echo "‚úì Continuing with cached data..."
            mkdir -p data/daily
            exit 0
          fi
          
          mkdir -p data/daily
          
          CACHED=0
          NEW=0
          
          # Download files
          for file in $POLYGONS $CSV; do
            [ -z "$file" ] && continue  # Skip empty lines
            
            fname=$(basename "$file")
            
            if [ -f "data/daily/$fname" ]; then
              # File already exists in cache
              CACHED=$((CACHED + 1))
            else
              # Download new file
              NEW=$((NEW + 1))
              if [ $NEW -le 5 ]; then
                echo "Downloading: $fname"
              fi
              
              curl -s -X POST https://content.dropboxapi.com/2/files/download \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Dropbox-API-Arg: {\"path\": \"$file\"}" \
                -o "data/daily/$fname"
            fi
          done
          
          echo ""
          echo "üìä Summary: $CACHED cached, $NEW new, $TOTAL total files"

      - name: Run R analysis
        run: |
          echo "Starting analysis..."
          [ ! -f "R/build_cumulative_hex.R" ] && echo "ERROR: Script not found" && exit 1
          
          # Check if we have any data files
          FILE_COUNT=$(find data/daily -name "*.geojson" 2>/dev/null | wc -l)
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "‚ö†Ô∏è  No data files found to process"
            echo "This might be the first run, or all scraping attempts failed"
            exit 1
          fi
          
          echo "Processing $FILE_COUNT data files..."
          Rscript R/build_cumulative_hex.R

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          keep_files: true
