name: Process 15-Minute HQ Outage Data with Persistent Cache

on:
  schedule:
    # Run every hour to process new 15-minute scraped data
    - cron: '0 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup R
        uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - name: System libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libgdal-dev libgeos-dev libproj-dev libudunits2-dev jq curl

      - name: Cache R packages
        uses: actions/cache@v3
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-r-packages-15min-v1
          restore-keys: ${{ runner.os }}-r-

      - name: Install R packages
        run: |
          Rscript -e 'pkgs <- c("sf","dplyr","fs","purrr","jsonlite","data.table"); inst <- setdiff(pkgs, rownames(installed.packages())); if(length(inst)) { cat("Installing", length(inst), "packages\n"); install.packages(inst, repos="https://cloud.r-project.org", Ncpus=2) } else { cat("All packages cached\n") }'

      - name: Download previous cache from public repo
        env:
          GITHUB_TOKEN: ${{ secrets.PUBLIC_REPO_TOKEN }}
        run: |
          echo "üì• Downloading previous cache from public repo..."
          
          # Try to download existing cache file
          curl -f -L \
            -H "Authorization: token $GITHUB_TOKEN" \
            -H "Accept: application/vnd.github.v3.raw" \
            "https://raw.githubusercontent.com/evelynebrie/hq-outages-public/gh-pages/cache/cumulative_hex_data.rds" \
            -o cache_download.rds 2>/dev/null || true
          
          if [ -f cache_download.rds ] && [ -s cache_download.rds ]; then
            mkdir -p cache
            mv cache_download.rds cache/cumulative_hex_data.rds
            echo "‚úÖ Cache downloaded successfully"
            ls -lh cache/cumulative_hex_data.rds
          else
            echo "‚ÑπÔ∏è  No existing cache found (this is normal for first run)"
          fi

      - name: Restore data cache
        uses: actions/cache@v3
        with:
          path: data/daily/
          key: hq-15min-data-${{ github.run_id }}
          restore-keys: hq-15min-data-

      - name: Restore hex grid cache
        uses: actions/cache@v3
        with:
          path: public/hex_grid_template.rds
          key: hq-hex-grid-15min-v1
          restore-keys: hq-hex-grid-

      - name: Get Fresh Dropbox Access Token
        id: get_token
        env:
          DROPBOX_REFRESH_TOKEN: ${{ secrets.DROPBOX_REFRESH_TOKEN }}
          DROPBOX_APP_KEY: ${{ secrets.DROPBOX_APP_KEY }}
          DROPBOX_APP_SECRET: ${{ secrets.DROPBOX_APP_SECRET }}
        run: |
          echo "üîë Getting fresh access token from refresh token..."
          
          RESPONSE=$(curl -s -X POST https://api.dropbox.com/oauth2/token \
            -u "${DROPBOX_APP_KEY}:${DROPBOX_APP_SECRET}" \
            -d "grant_type=refresh_token&refresh_token=${DROPBOX_REFRESH_TOKEN}")
          
          ACCESS_TOKEN=$(echo "$RESPONSE" | jq -r '.access_token // empty')
          
          if [ -z "$ACCESS_TOKEN" ]; then
            echo "‚ùå Failed to get access token. Response:"
            echo "$RESPONSE" | jq '.'
            exit 1
          fi
          
          echo "‚úÖ Successfully obtained fresh access token"
          echo "access_token=$ACCESS_TOKEN" >> $GITHUB_OUTPUT

      - name: Download files from Dropbox App Folder
        env:
          DROPBOX_TOKEN: ${{ steps.get_token.outputs.access_token }}
        run: |
          set -eo pipefail
          
          echo "üì• Listing files from Dropbox App Folder..."
          echo "   Path: /hq-outages (App Folder root)"
          echo ""
          
          ALL_FILES=""
          HAS_MORE=true
          CURSOR=""
          PAGE=1
          
          # List all files recursively from app folder
          while [ "$HAS_MORE" = "true" ]; do
            echo "  Fetching page $PAGE..."
            
            if [ -z "$CURSOR" ]; then
              # App folders use root path "" or the folder name
              RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Content-Type: application/json" \
                -d '{"path": "", "recursive": true}')
            else
              RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder/continue \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Content-Type: application/json" \
                -d "{\"cursor\": \"$CURSOR\"}")
            fi
            
            if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
              echo "‚ùå Error from Dropbox API:"
              echo "$RESPONSE" | jq '.error'
              
              # Try alternative path
              echo ""
              echo "Trying alternative path: /hq-outages"
              RESPONSE=$(curl -s -X POST https://api.dropboxapi.com/2/files/list_folder \
                -H "Authorization: Bearer $DROPBOX_TOKEN" \
                -H "Content-Type: application/json" \
                -d '{"path": "/hq-outages", "recursive": true}')
              
              if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
                echo "‚ùå Still failed. Please check Dropbox configuration."
                exit 1
              fi
            fi
            
            if ! echo "$RESPONSE" | jq -e '.entries' > /dev/null 2>&1; then
              echo "‚ö†Ô∏è  Unexpected response - continuing with empty list"
              break
            fi
            
            PAGE_COUNT=$(echo "$RESPONSE" | jq '.entries | length')
            echo "    Found $PAGE_COUNT entries"
            
            PAGE_FILES=$(echo "$RESPONSE" | jq -r '.entries[]? | select(.".tag" == "file") | .path_display' || true)
            
            if [ ! -z "$PAGE_FILES" ]; then
              if [ -z "$ALL_FILES" ]; then
                ALL_FILES="$PAGE_FILES"
              else
                ALL_FILES="$ALL_FILES"$'\n'"$PAGE_FILES"
              fi
            fi
            
            HAS_MORE=$(echo "$RESPONSE" | jq -r '.has_more')
            
            if [ "$HAS_MORE" = "true" ]; then
              CURSOR=$(echo "$RESPONSE" | jq -r '.cursor')
              PAGE=$((PAGE + 1))
            fi
          done
          
          echo "‚úÖ Completed pagination: $PAGE page(s)"
          
          TOTAL_COUNT=0
          if [ ! -z "$ALL_FILES" ]; then
            TOTAL_COUNT=$(echo "$ALL_FILES" | wc -l)
          fi
          echo "üìä Total files found: $TOTAL_COUNT"
          
          if [ $TOTAL_COUNT -eq 0 ]; then
            echo "‚ö†Ô∏è  No files found - continuing with cached data"
            mkdir -p data/daily
            exit 0
          fi
          
          # ============================================================
          # IMPORTANT: Download POLYGON files (actual outage areas)
          # These contain the polygon boundaries, not just point markers
          # ============================================================
          echo ""
          echo "üîç Looking for POLYGON files (polygons_*.geojson) - these contain outage AREAS"
          
          POLYGON_FILES=$(echo "$ALL_FILES" | grep -E "polygons_[0-9]{8}[Tt][0-9]{6}\.geojson$" || true)
          POLYGON_COUNT=$(echo "$POLYGON_FILES" | grep -v '^$' | wc -l)
          
          echo "üìç Polygon files found: $POLYGON_COUNT"
          
          # Also check for joined files as fallback
          echo ""
          echo "üîç Also checking for joined point files (fallback)"
          JOINED_FILES=$(echo "$ALL_FILES" | grep "outages_joined_full_" | grep -E "[0-9]{8}[Tt][0-9]{6}\.geojson$" || true)
          JOINED_COUNT=$(echo "$JOINED_FILES" | grep -v '^$' | wc -l)
          echo "üìç Joined point files found: $JOINED_COUNT"
          
          # Prefer polygon files, fall back to joined files
          if [ "$POLYGON_COUNT" -gt 0 ]; then
            echo ""
            echo "‚úÖ Using POLYGON files (recommended - contains actual outage areas)"
            GEOJSON_FILES="$POLYGON_FILES"
            FILE_COUNT=$POLYGON_COUNT
          elif [ "$JOINED_COUNT" -gt 0 ]; then
            echo ""
            echo "‚ö†Ô∏è  No polygon files found, using joined point files as fallback"
            echo "   Note: Point files are less accurate - they only show markers, not full areas"
            GEOJSON_FILES="$JOINED_FILES"
            FILE_COUNT=$JOINED_COUNT
          else
            echo ""
            echo "‚ùå No matching files found!"
            echo ""
            echo "Sample of all files found (first 20):"
            echo "$ALL_FILES" | head -20
            echo ""
            echo "Expected patterns:"
            echo "  - polygons_YYYYMMDDTHHMMSS.geojson (preferred)"
            echo "  - outages_joined_full_YYYYMMDDTHHMMSS.geojson (fallback)"
            mkdir -p data/daily
            exit 0
          fi
          
          # Show sample
          echo ""
          echo "Sample files to download (first 5):"
          echo "$GEOJSON_FILES" | head -5
          echo ""
          
          mkdir -p data/daily
          
          CACHED=0
          NEW=0
          SKIPPED=0
          
          # Download files
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            filename=$(basename "$file")
            local_path="data/daily/$filename"
            
            # Skip if already downloaded
            if [ -f "$local_path" ] && [ -s "$local_path" ]; then
              CACHED=$((CACHED + 1))
              continue
            fi
            
            # Download new file
            if curl -s -f -X POST https://content.dropboxapi.com/2/files/download \
              -H "Authorization: Bearer $DROPBOX_TOKEN" \
              -H "Dropbox-API-Arg: {\"path\": \"$file\"}" \
              -o "$local_path" && [ -s "$local_path" ]; then
              NEW=$((NEW + 1))
              [ $NEW -le 10 ] && echo "  ‚úì Downloaded: $filename"
            else
              SKIPPED=$((SKIPPED + 1))
              [ $SKIPPED -le 5 ] && echo "  ‚úó Failed: $filename"
            fi
            
            # Rate limiting: pause briefly every 10 downloads
            if [ $((NEW % 10)) -eq 0 ] && [ $NEW -gt 0 ]; then
              sleep 1
            fi
          done <<< "$GEOJSON_FILES"
          
          echo ""
          echo "üìä Download Summary:"
          echo "  ‚Ä¢ Cached (already present): $CACHED"
          echo "  ‚Ä¢ Newly downloaded: $NEW"
          echo "  ‚Ä¢ Skipped/failed: $SKIPPED"
          echo "  ‚Ä¢ Total available: $(($CACHED + $NEW))"
          
          # Show what we have
          echo ""
          echo "üìÅ Files in data/daily/:"
          ls -la data/daily/*.geojson 2>/dev/null | head -10 || echo "  No files yet"

      - name: Clear old cache if rebuilding (ONE-TIME FIX)
        run: |
          # Check if we need to rebuild from polygon data
          # This clears the cache if it was built from point data
          if [ -f "cache/cumulative_hex_data.rds" ]; then
            # Check if we have polygon files now
            POLYGON_COUNT=$(ls data/daily/polygons_*.geojson 2>/dev/null | wc -l)
            if [ "$POLYGON_COUNT" -gt 0 ]; then
              echo "üîÑ Polygon files detected - clearing old cache to rebuild with correct data"
              rm -f cache/cumulative_hex_data.rds
              echo "‚úÖ Cache cleared - will rebuild from polygon data"
            fi
          fi

      - name: Run 15-minute scrape analysis (processing only)
        run: |
          echo "üî¨ Starting 15-minute scrape analysis with cache support..."
          
          if [ ! -f "R/build_cumulative_hex_15min.R" ]; then
            echo "‚ùå Script not found at R/build_cumulative_hex_15min.R"
            exit 1
          fi
          
          FILE_COUNT=$(find data/daily -name "*.geojson" 2>/dev/null | wc -l)
          echo "üìä Processing $FILE_COUNT total files"
          
          # Show file types
          echo ""
          echo "üìÅ File breakdown:"
          POLYGON_COUNT=$(ls data/daily/polygons_*.geojson 2>/dev/null | wc -l || echo 0)
          JOINED_COUNT=$(ls data/daily/outages_joined_full_*.geojson 2>/dev/null | wc -l || echo 0)
          echo "  ‚Ä¢ Polygon files: $POLYGON_COUNT"
          echo "  ‚Ä¢ Joined point files: $JOINED_COUNT"
          
          if [ -f "cache/cumulative_hex_data.rds" ]; then
            echo "‚úÖ Cache file found - will process only new files!"
          else
            echo "‚ÑπÔ∏è  No cache - will process all files (first run or rebuild)"
          fi
          
          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "‚ùå No data to process!"
            exit 1
          fi
          
          # Show sample of files being processed
          echo ""
          echo "Sample files:"
          ls -lh data/daily/*.geojson 2>/dev/null | head -5
          
          # Run the R script - this includes HTML generation
          # If it fails, we still want to save the cache before it
          Rscript R/build_cumulative_hex_15min.R || {
            echo "‚ö†Ô∏è  R script failed, but continuing to save cache..."
            exit_code=$?
            
            # Copy cache even if script failed
            if [ -f "cache/cumulative_hex_data.rds" ]; then
              echo "üíæ Saving cache despite error..."
              mkdir -p public/cache
              cp cache/cumulative_hex_data.rds public/cache/
              echo "‚úÖ Cache saved for next run"
            fi
            
            exit $exit_code
          }

      - name: Copy cache to public directory for deployment
        if: success()
        run: |
          echo "üì¶ Preparing cache for deployment..."
          mkdir -p public/cache
          
          if [ -f "cache/cumulative_hex_data.rds" ]; then
            cp cache/cumulative_hex_data.rds public/cache/
            ls -lh public/cache/cumulative_hex_data.rds
            echo "‚úÖ Cache copied to public/cache/"
          else
            echo "‚ö†Ô∏è  No cache file found to copy"
          fi

      - name: Generate summary statistics
        run: |
          echo "üìä Generating summary statistics..."
          
          if [ -f "R/generate_summaries.R" ]; then
            Rscript R/generate_summaries.R
          else
            echo "‚ö†Ô∏è  generate_summaries.R not found, skipping statistics generation"
          fi

      - name: Verify outputs were created
        run: |
          echo "üìÅ Verifying generated files..."
          
          # Check for HTML (warn if missing, but don't fail)
          if [ ! -f "public/index.html" ]; then
            echo "‚ö†Ô∏è  WARNING: index.html not created (HTML generation may have failed)"
            echo "   Data files are still valid and will be deployed"
          else
            echo "‚úÖ index.html created successfully"
          fi
          
          # Check for critical data file
          if [ ! -f "public/total/total_exposure.geojson" ]; then
            echo "‚ùå CRITICAL: total_exposure.geojson not created!"
            exit 1
          fi
          
          DAILY_COUNT=$(find public/daily -name "daily_*.geojson" 2>/dev/null | wc -l)
          MONTHLY_COUNT=$(find public/monthly -name "monthly_*.geojson" 2>/dev/null | wc -l)
          SNAPSHOT_COUNT=$(find public/cumulative_snapshots -name "cumulative_*.geojson" 2>/dev/null | wc -l)
          CACHE_EXISTS=$([ -f public/cache/cumulative_hex_data.rds ] && echo 'Yes ‚úÖ' || echo 'No ‚ùå')
          
          echo ""
          echo "üìä Generated files:"
          echo "  ‚Ä¢ HTML visualization: $([ -f public/index.html ] && echo 'Yes ‚úÖ' || echo 'No ‚ö†Ô∏è ')"
          echo "  ‚Ä¢ Total exposure: Yes ‚úÖ"
          echo "  ‚Ä¢ Daily summaries: $DAILY_COUNT"
          echo "  ‚Ä¢ Monthly summaries: $MONTHLY_COUNT"
          echo "  ‚Ä¢ Cumulative snapshots: $SNAPSHOT_COUNT"
          echo "  ‚Ä¢ Cache file: $CACHE_EXISTS"
          
          if [ "$DAILY_COUNT" -eq 0 ]; then
            echo "‚ö†Ô∏è  Warning: No daily summaries created"
          fi
          
          echo ""
          echo "üìä Public directory structure:"
          du -sh public/*/ 2>/dev/null || echo "No subdirectories"
          
          echo ""
          if [ -f "public/index.html" ]; then
            echo "‚úÖ All files verified successfully"
          else
            echo "‚ö†Ô∏è  Data files verified - HTML missing but deployment will continue"
            echo "   You can fix the HTML generation bug and re-run"
            echo "   Cache is saved, so next run will be fast!"
          fi

      - name: Display summary statistics
        run: |
          echo "üìä Summary Statistics:"
          echo ""
          
          if [ -f "public/summaries/summary_report.txt" ]; then
            head -30 public/summaries/summary_report.txt
          else
            echo "Summary report not available"
          fi

      - name: Deploy to PUBLIC repo (gh-pages) with cache
        uses: peaceiris/actions-gh-pages@v3
        with:
          personal_token: ${{ secrets.PUBLIC_REPO_TOKEN }}
          external_repository: evelynebrie/hq-outages-public
          publish_branch: gh-pages
          publish_dir: public
          keep_files: false
          commit_message: "Update 15-minute scrape data - ${{ github.run_number }}"
          
      - name: Summary
        run: |
          echo ""
          echo "========================================="
          echo "‚úÖ DEPLOYMENT COMPLETE"
          echo "========================================="
          echo ""
          echo "üåê View at: https://evelynebrie.github.io/hq-outages-public/"
          echo ""
          
          DAILY_COUNT=$(find public/daily -name "daily_*.geojson" 2>/dev/null | wc -l)
          MONTHLY_COUNT=$(find public/monthly -name "monthly_*.geojson" 2>/dev/null | wc -l)
          CACHE_STATUS=$([ -f public/cache/cumulative_hex_data.rds ] && echo "‚úÖ Deployed" || echo "‚ùå Not found")
          
          echo "üìä Deployment summary:"
          echo "  ‚Ä¢ Daily summaries: $DAILY_COUNT"
          echo "  ‚Ä¢ Monthly summaries: $MONTHLY_COUNT"
          echo "  ‚Ä¢ Cache status: $CACHE_STATUS"
          echo "  ‚Ä¢ Processing: 15-minute interval data"
          echo "  ‚Ä¢ Next run will use deployed cache"
          echo "========================================="
